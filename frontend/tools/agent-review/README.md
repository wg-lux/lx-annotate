# Agent Review Tool â€” Vue 3 + Pinia + Vitest

Ein systematisches DX/QA-Review-Tool fÃ¼r Vue 3 + Pinia + Vitest Test-Setups mit strukturierter JSON-Schema-Bewertung.

## ðŸŽ¯ Features

- **Automatisierte Artefakt-Sammlung**: Vitest JSON, Coverage Reports, Fail-Logs, Setup-Snippets
- **Strukturierte Agent-Analyse**: JSON-Schema-basierte Bewertung mit Scoring-Algorithmus
- **PrÃ¤zise Problem-Identifikation**: Kategorisierte Findings mit Evidence und Impact-Analyse
- **Actionable Fixes**: Minimale Patches mit ETA und Dependency-Tracking
- **Dual CLI**: TypeScript (Node) und Python Implementierung

## ðŸ“ Struktur

```
tools/agent-review/
â”œâ”€â”€ README.md                 # Diese Dokumentation
â”œâ”€â”€ agent-spec.md            # PrÃ¤zise Agent-Prompt-Spezifikation
â”œâ”€â”€ examples/                # Beispiel-Artefakte und -Outputs
â”‚   â”œâ”€â”€ vitest-example.json
â”‚   â”œâ”€â”€ coverage-example.json
â”‚   â””â”€â”€ agent-output-example.json
â”œâ”€â”€ cli/                     # CLI Implementierungen
â”‚   â”œâ”€â”€ node/               # TypeScript/Node CLI
â”‚   â”‚   â”œâ”€â”€ package.json
â”‚   â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â”œâ”€â”€ cli.ts
â”‚   â”‚       â”œâ”€â”€ collect.ts
â”‚   â”‚       â”œâ”€â”€ validate.ts
â”‚   â”‚       â””â”€â”€ schema.ts
â”‚   â””â”€â”€ python/             # Python CLI (single file)
â”‚       â””â”€â”€ agent_cli.py
â””â”€â”€ scripts/                # Convenience scripts
    â”œâ”€â”€ full-review.sh      # End-to-end review workflow
    â””â”€â”€ validate-setup.sh   # Test setup validation
```

## ðŸš€ Quick Start

### Option A: Vollautomatischer Workflow

```bash
# End-to-end Review in einem Schritt
bash tools/agent-review/scripts/full-review.sh

# Dann: LLM mit build/prompt.txt laufen lassen â†’ agent_output.json speichern
# Validieren: python3 tools/agent-review/cli/python/agent_cli.py validate --output agent_output.json
```

### Option B: Manuelle Schritte

### 1. Artefakte sammeln

```bash
# Vitest JSON Report generieren
npx vitest run --reporter=json > vitest.json

# Coverage Report (optional)
npx vitest run --coverage
# â†’ erzeugt coverage/coverage-summary.json

# Python CLI verwenden
python3 tools/agent-review/cli/python/agent_cli.py collect \
  --vitest vitest.json \
  --coverage coverage/coverage-summary.json \
  --faillog fail.log \
  --snippets setup_snippets.txt
```

### 2. Agent-Prompt generieren

```bash
python3 tools/agent-review/cli/python/agent_cli.py prompt
# â†’ build/prompt.txt
```

### 3. Agent-Analyse durchfÃ¼hren

Verwende den generierten Prompt mit deinem LLM und speichere die Antwort als `agent_output.json`.

### 4. Output validieren

```bash
# Mit eigenem Agent-Output (nachdem LLM gelaufen ist)
python3 tools/agent-review/cli/python/agent_cli.py validate \
  --output agent_output.json

# Demo mit Beispiel-Output
python3 tools/agent-review/cli/python/agent_cli.py validate \
  --output tools/agent-review/examples/agent-output-example.json
```

## ðŸ“‹ Agent-Spec Schema

Das Tool verwendet ein prÃ¤zises JSON-Schema fÃ¼r strukturierte Reviews:

```json
{
  "summary": "Kurze 6-Satz Zusammenfassung",
  "scores": {
    "pass_rate_pct": 72.73,
    "coverage_lines_pct": 71.11,
    "setup_correctness_pct": 65,
    "determinism_pct": 60,
    "overall_pct": 67.95
  },
  "verdict": "pass | soft-fail | fail | blocked",
  "findings": [{
    "id": "F1",
    "category": "pinia-setup",
    "severity": "high",
    "evidence": "konkreter Beleg aus Artefakten",
    "impact": "warum es Tests bricht",
    "fix": {
      "type": "minimal-patch",
      "patch": "â‰¤25 Zeilen Code-Fix",
      "notes": "Hinweise/Alternativen"
    }
  }],
  "actions": [{
    "priority": 1,
    "title": "Implementiere createTestingPinia",
    "eta_minutes": 15,
    "depends_on": [],
    "details": "konkrete Umsetzung"
  }]
}
```

## ðŸŽ¯ Bewertungskriterien

### Scoring-Algorithmus
- **overall_pct** = 0.30Ã—pass_rate + 0.25Ã—coverage_lines + 0.25Ã—setup_correctness + 0.20Ã—determinism

### setup_correctness_pct (max 100)
- +30: Globale Provider korrekt (Pinia, Router, etc.)
- +25: Gemeinsames mount utility
- +25: Kein vi.mock Hoisting-Problem
- +20: Deterministische Store-Mocks

### determinism_pct (max 100)
- +40: Statische Mocks (keine Random/Time-Dependencies)
- +30: Keine Zeit/Zufall-Flakes
- +30: Konsistente Test-Fixtures

### Verdict
- **blocked**: Kritische Artefakte fehlen
- **fail**: overall < 60%
- **soft-fail**: 60-79.99%
- **pass**: â‰¥ 80%

## ðŸ”§ CLI Reference

### Python CLI

```bash
# Artefakte sammeln
python3 agent_cli.py collect \
  [--vitest vitest.json] \
  [--coverage coverage/coverage-summary.json] \
  [--faillog fail.log] \
  [--snippets setup.txt] \
  [--out build/artifacts.json]

# Prompt generieren
python3 agent_cli.py prompt \
  [--artifacts build/artifacts.json] \
  [--out build/prompt.txt]

# Output validieren
python3 agent_cli.py validate --output agent_output.json
```

### TypeScript CLI

```bash
# Setup (einmalig)
cd tools/agent-review/cli/node
npm install

# Verwendung
ts-node src/cli.ts collect [options]
ts-node src/cli.ts prompt [options]
ts-node src/cli.ts validate --output agent_output.json
```

## ðŸ“ Kategorien & Findings

### Supported Categories
- **pinia-setup**: Store-Provider, createTestingPinia, initialState
- **vi.mock-hoisting**: Module-Hoisting, Top-level mocks
- **global-providers**: Mount-Konfiguration, Plugins, Stubs
- **coverage**: Test-Coverage, ungetestete Branches
- **determinism**: Flaky tests, Time/Random dependencies
- **other**: Sonstige Setup-Probleme

### Severity Levels
- **high**: Kritische Probleme die Tests brechen
- **medium**: Wichtige Verbesserungen fÃ¼r StabilitÃ¤t
- **low**: Nice-to-have Optimierungen

## ðŸŽ­ Beispiel-Workflow

```bash
#!/bin/bash
# full-review.sh - Kompletter Review-Workflow

echo "ðŸ§ª Running tests with JSON output..."
npx vitest run --reporter=json > vitest.json

echo "ðŸ“Š Generating coverage report..."
npx vitest run --coverage --silent >/dev/null 2>&1

echo "ðŸ“‹ Collecting artifacts..."
python3 tools/agent-review/cli/python/agent_cli.py collect \
  --vitest vitest.json \
  --coverage coverage/coverage-summary.json

echo "ðŸ¤– Generating agent prompt..."
python3 tools/agent-review/cli/python/agent_cli.py prompt

echo "ðŸ“ Prompt ready at: build/prompt.txt"
echo "ðŸ“¤ Run your LLM with this prompt and save output as agent_output.json"
echo "âœ… Then validate with: python3 tools/agent-review/cli/python/agent_cli.py validate --output agent_output.json"
```

## ðŸ’¡ Best Practices

### Artefakt-QualitÃ¤t sicherstellen
- **Vitest JSON**: `npx vitest run --reporter=json` (robusteste Methode)
- **Coverage**: Nutze `coverage/coverage-summary.json` (Istanbul), nicht text-parsing
- **Fail-Logs**: `npx vitest run --reporter=verbose 2>&1 | tee fail.log`
- **Setup-Snippets**: Relevante Ausschnitte aus mount utils, vi.mock blocks

### HÃ¤ufige Probleme
- **Worker Module Error**: Versuche `npx vitest run` statt `npm run test:unit`
- **Leere JSON Files**: PrÃ¼fe ob Tests tatsÃ¤chlich laufen
- **Bash Shebang**: Nutze `bash script.sh` statt `./script.sh` in Nix-Umgebungen

### Agent-Prompting
- Verwende die generierte `build/prompt.txt` exakt
- Agent soll **nur** JSON zurÃ¼ckgeben (kein zusÃ¤tzlicher Text)
- Bei Schema-Fehlern: erneut prompten mit Fehlermeldung

### Integration in CI/CD
```yaml
# .github/workflows/test-review.yml
- name: Generate test review
  run: |
    npm run test:unit -- --reporter=json > vitest.json
    python3 tools/agent-review/cli/python/agent_cli.py collect
    python3 tools/agent-review/cli/python/agent_cli.py prompt
    # LLM API call hier
    python3 tools/agent-review/cli/python/agent_cli.py validate --output agent_output.json
```

## ðŸ”— Vue 3 + Pinia Spezifika

### Typische Findings
- **createTestingPinia vs vi.mock()**: Store-Mock-Integration
- **initialState**: Deterministische Store-ZustÃ¤nde
- **Global Mount**: Pinia Provider in mount() config
- **Hoisting**: vi.mock() Positionierung fÃ¼r Vitest

### LÃ¶sungspatterns
```typescript
// âœ… Correct: createTestingPinia mit initialState
const pinia = createTestingPinia({
  initialState: {
    patient: { patients: [...] },
    examination: { examinations: [...] }
  }
});

// âŒ Problematic: vi.mock() fÃ¼r Pinia stores
vi.mock('@/stores/patientStore', () => ({ ... }));
```

## ðŸ“ˆ Erfolgsmetriken

- **Pass Rate**: >90% fÃ¼r production-ready tests
- **Coverage**: >80% Lines/Statements
- **Setup Correctness**: >80% (korrekte Provider, utils, mocks)
- **Determinism**: >90% (keine Flakes, statische fixtures)
- **Overall Score**: >80% fÃ¼r "pass" verdict

Das Tool hilft dabei, diese Metriken systematisch zu erreichen und zu Ã¼berwachen.
